{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports, Reading in files & Defining functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     29,
     46,
     88,
     189,
     196
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import regex as re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from termcolor import colored\n",
    "from colorama import Back, Style\n",
    "\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "\n",
    "from autocorrect import Speller # Spell checker\n",
    "import shutil\n",
    "import nltk\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = [word.upper() for word in stopwords.words('english')]\n",
    "hard_coded_non_section_words = ['URL', 'UTC', 'JSTOR', 'AMERICAN', 'JOURNAL', 'SOCIOLOGY', 'ABSTRACT', 'TABLE', 'CHART',\n",
    "                                'AMERICAN JOURNAL OF SOCIOLOGY', 'UNIVERSITY OF CHICAGO', 'AMERICA', 'JOURNAL OF SOCIOLOGY',\n",
    "                                'REFERENCES', 'AMERICAN SOCIOLOGICAL REVIEW', 'AMERICAN SOCIOLOGICAL ASSOCIATION', 'AMERICAN',\n",
    "                                'AMERICAN JOURNAL', 'UNIVERSITY', 'CHICAGO', 'MERICA JOURNAL', 'AMERICAN OURNAL', 'MERICA JOURNA', \n",
    "                                'SAGE', ]\n",
    "\n",
    "def PDFtoString(filePath, pdfFolder=None):\n",
    "    \n",
    "    out = io.StringIO()\n",
    "    if pdfFolder is not None:\n",
    "        filePath = os.path.join(pdfFolder, filePath)\n",
    "    with open(filePath, 'rb') as f:\n",
    "        parser = PDFParser(f)\n",
    "        doc = PDFDocument(parser)\n",
    "        rsrcmgr = PDFResourceManager()\n",
    "        device = TextConverter(rsrcmgr, out, laparams=LAParams())\n",
    "        interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "        for indx, page in enumerate(PDFPage.create_pages(doc)):\n",
    "            #if indx != 0:\n",
    "            interpreter.process_page(page)\n",
    "            \n",
    "    return out.getvalue() \n",
    "\n",
    "def printMetaInfo(convertedStrings, pdfPaths, journal = 'AJS'):\n",
    "    \n",
    "    # Print the regex matches for access date, abstract, table/chart, and references\n",
    "    for indx, string in enumerate(convertedStrings):\n",
    "        print(indx, colored(pdfPaths[indx], 'red'))\n",
    "        if re.findall(r'Accessed: \\d{2}-\\d{2}-\\d{4} \\d{2}:\\d{2} UTC', string):\n",
    "            print('\\t', colored(re.findall(r'Accessed: \\d{2}-\\d{2}-\\d{4} \\d{2}:\\d{2} UTC', string)[0], 'blue'))\n",
    "        print('\\t', colored('Abstract match:', 'magenta'), colored(re.search(r'ABSTRACT', string), 'magenta'))\n",
    "        \n",
    "        # Chart, table, or figure matches\n",
    "        matches = re.finditer(r'CHART', string)\n",
    "        for match in matches:\n",
    "            print('\\t', colored('Chart match:', 'green'), colored(match, 'green'))\n",
    "        matches = re.finditer(r'TABLE', string)\n",
    "        for match in matches:\n",
    "            print('\\t', colored('Table match:', 'green'), colored(match, 'green'))\n",
    "        matches = re.finditer(r'FIG|FIGURE|Fig', string)\n",
    "        for match in matches:\n",
    "            print('\\t', colored('Figure match:', 'green'), colored(match, 'green'))  \n",
    "            \n",
    "        print('\\t', colored('Reference match:', 'magenta'), colored(re.search(r'REFERENCES', string), 'magenta'))\n",
    "        \n",
    "        # This finds the title words which aren't stop words & aren't digits and upper cases them all\n",
    "        if journal == 'AJS':\n",
    "            titles = [re.findall(r'(?<=AJS_\\d{4}_\\d{1,3}_\\d{1,2}_).*(?=.pdf)', file_name)[0] for file_name in pdfPaths]\n",
    "        elif journal == 'ASR':\n",
    "            titles = [re.findall(r'(?<=ASR_\\d{4}_\\d{1,3}_\\d{1,2}_).*(?=.pdf)', file_name)[0] for file_name in pdfPaths]\n",
    "        title_words = [i for i in titles[indx].upper().split() if i not in stop_words and not i.isdigit()]\n",
    "        \n",
    "        # Then adds them to stop words & other hard-coded regularly occuring words\n",
    "        non_section_words = hard_coded_non_section_words + title_words + stop_words \n",
    "        non_section_words += [title.upper() for title in titles] + [title.strip('The ').upper() for title in titles]\n",
    "        \n",
    "        matches = re.finditer(r'[A-Z]{4,}(\\s+?[A-Z]{2,}){0,}', string)\n",
    "        sections = dict()\n",
    "        \n",
    "        for match in matches:\n",
    "            if match.group() not in non_section_words and match not in sections:\n",
    "                sections[match.group()] = match.span()\n",
    "        for section in sections:\n",
    "            print('\\t', colored('Section match:', 'blue'), colored([section, sections[section]], 'blue'))\n",
    "\n",
    "def createOutputStrings(convertedStrings, folder = None, journal = 'AJS'):\n",
    "    out = list() #Output strings go here, hopefully one out per pdf s.t. len(out) == len(convertedStrings)\n",
    "    for indx, string in enumerate(convertedStrings):\n",
    "        \n",
    "        # Define meta-information\n",
    "        outString = '-------------\\n||Meta-info||\\n-------------\\n'\n",
    "\n",
    "        # The second condition is more restrictive, so check that that's not None.\n",
    "        header = article = None\n",
    "        if re.search(r'extend access to\\n(.)+?(?=C)(.)+?(?=\\w{2,})', string) is not None:\n",
    "            header = string[:re.search(r'extend access to\\n(.)+?(?=C)', string).end()].strip()\n",
    "            if re.search(r' Your use of the JSTOR', header) is not None:\n",
    "                header = header[:re.search(r' Your use of the JSTOR', header).span()[0]]\n",
    "            article = string[re.search(r'extend access to\\n(.)+?(?=C)(.)+?(?=\\w{2,})', string).end():]\n",
    "            outString += re.sub(r'Accessed: \\d{2}-\\d{2}-\\d{4} \\d{2}:\\d{2} UTC', '', header)\n",
    "        \n",
    "        # Remove junk at the beginning of AJS 1946 to 1966 papers \n",
    "        junk_string = r'Pale AM ee ee ee at eta\\nare now only accessible on(\\s*)?(the Chicago Journals website at)?(\\s*)?(EVR LeU|Pee ot AR eee ea aaa\\nare now only accessible on)?(\\s*)?(Pee ot AR eee ea aaa\\nare now only accessible on)?(\\s*)?(the Chicago Journals website at)?'\n",
    "        if re.search(junk_string, string) is not None:\n",
    "            string = re.compile(junk_string).split(string)[-1]\n",
    "        \n",
    "        # Remove junk at the beginning of ASR papers\n",
    "        junk_stringASR = r'(@SAGE As\\)A\\s*)?Sage Publications, Inc., American Sociological Association are collaborating with STR to digitized,\\npreserve and extend access to American Sociological Review'\n",
    "        junk_stringASR += '|American Sociological Association, Sage Publications, Inc. are collaborating with STR to digitized,\\npreserve and extend access to American Sociological Review\\nSAGE'\n",
    "\n",
    "        if re.search(junk_stringASR, string) is not None:\n",
    "            string = re.compile(junk_stringASR).split(string)[-1]\n",
    "            \n",
    "        # Add access date\n",
    "        access_date = re.findall(r'Accessed: \\d{2}-\\d{2}-\\d{4} \\d{2}:\\d{2} UTC', string)\n",
    "        if len(access_date) != 0:\n",
    "            outString += '\\n\\n' + access_date[0] + '\\n'\n",
    "            \n",
    "        # Add abstract info\n",
    "        abstract = re.search(r'ABSTRACT', string)\n",
    "        if abstract is not None:\n",
    "            outString += 'Abstract match at {}'.format(abstract.span()) + '\\n'\n",
    "        elif abstract is None:\n",
    "            outString += 'No abstract found.\\n'\n",
    "\n",
    "        # Add chart info\n",
    "        chart_matches = re.finditer(r'CHART', string)    \n",
    "        if chart_matches is not None:\n",
    "            for match in chart_matches:\n",
    "                outString += 'Chart match at {}'.format(match.span()) + '\\n'\n",
    "\n",
    "        # Add table info \n",
    "        table_matches = re.finditer(r'TABLE', string)\n",
    "        if table_matches is not None:\n",
    "            for match in table_matches:\n",
    "                outString += 'Chart match at {}'.format(match.span()) + '\\n'\n",
    "                \n",
    "        # Add figure info\n",
    "        figure_matches = re.finditer(r'FIG|FIGURE|Fig', string)\n",
    "        if figure_matches is not None:\n",
    "            for match in figure_matches:\n",
    "                outString += 'Figure match at {}'.format(match.span()) + '\\n'\n",
    "\n",
    "        # Add references info\n",
    "        references = re.search(r'REFERENCES', string)\n",
    "        if references is not None:\n",
    "            outString += 'Reference match at {}'.format(references.span()) + '\\n'\n",
    "        elif references is None:\n",
    "            outString += 'No references section found.\\n'\n",
    "\n",
    "        # Add section info \n",
    "\n",
    "        ### First, look for the title words and tokenize them.\n",
    "        if journal == 'AJS':\n",
    "            titles = [re.findall(r'(?<=AJS_\\d{4}_\\d{1,3}_\\d{1,2}_).*(?=.pdf)', file_name)[0] for file_name in folder]\n",
    "        elif journal == 'ASR':\n",
    "            titles = [re.findall(r'(?<=ASR_\\d{4}_\\d{1,3}_\\d{1,2}_).*(?=.pdf)', file_name)[0] for file_name in folder]\n",
    "        title_words = [i for i in titles[indx].upper().split() if i not in stop_words and not i.isdigit()]\n",
    "\n",
    "        ### Then add them to stop words & other hard-coded regularly occuring words\n",
    "        non_section_words = hard_coded_non_section_words + title_words + stop_words\n",
    "        matches = re.finditer(r'[A-Z]{4,}(\\s+?[A-Z]{2,}){0,}', string)\n",
    "\n",
    "        if matches is not None:\n",
    "            sections = dict()\n",
    "            for match in matches:\n",
    "                if match.group() not in non_section_words and match not in sections:\n",
    "                    sections[match.group()] = match.span()\n",
    "        for section in sections:\n",
    "            outString += 'Section header \"{}\" found at {}'.format(section, sections[section]) + '\\n'\n",
    "            if article is not None:\n",
    "                article = re.sub(section, section + '.', article) # Add '.' to end of sections because Dr. Franzosi's\n",
    "            elif article is None: \n",
    "                string = re.sub(section, section + '.', string) # section parser identifies section headers with it\n",
    "\n",
    "        # Add in the article itself\n",
    "        outString += '\\n-----------\\n||Article||\\n-----------\\n'\n",
    "        if article is not None:\n",
    "            outString += article\n",
    "        elif article is None: # If regex wasn't able to split it according to the JSTOR access message just put the full article\n",
    "            outString += string\n",
    "            \n",
    "        # Append the newly made string to the list. Again, there should be 1 per pdf\n",
    "        out.append(outString)\n",
    "    return out\n",
    "\n",
    "def writeOut(out, pdfFolder = None, outFolder = None):\n",
    "    for indx, file in tqdm(enumerate(out)):\n",
    "        writeFilePath = 'corpus/{}/{}.txt'.format(outFolder, pdfFolder[indx][:-4])\n",
    "        with open(writeFilePath, 'w') as f:\n",
    "            f.write(file)\n",
    "    print('done!')\n",
    "\n",
    "def highlight(pattern, text, printOut = False):\n",
    "    output = text\n",
    "    lookforward = 0\n",
    "    for match in pattern.finditer(text):\n",
    "        start, end = match.start() + lookforward, match.end() + lookforward\n",
    "        output = output[:start] + Back.YELLOW + Style.BRIGHT + output[start:end] + Style.RESET_ALL + output[end:]\n",
    "        lookforward = len(output) - len(text)  \n",
    "\n",
    "    if printOut:\n",
    "        print(output)\n",
    "    else:\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------\n",
      "||Meta-info||\n",
      "-------------\n",
      "Moving Teenagers Out of High-Risk Neighborhoods: How Girls Fare Better than Boys\n",
      "Author(s): Susan Clampet-Lundquist, Kathryn Edin, Jeffrey R. Kling and  Greg J. Duncan\n",
      "Source: American Journal of Sociology, Vol. 116, No. 4 (January 2011), pp. 1154-89\n",
      "Published by: The University of Chicago Press\n",
      "Stable URL: http://www.jstor.org/stable/10.1086/657352\n",
      "\n",
      "\n",
      "Accessed: 08-06-2016 04:54 UTC\n",
      "No abstract found.\n",
      "Chart match at (31510, 31515)\n",
      "Chart match at (90528, 90533)\n",
      "Figure match at (23265, 23268)\n",
      "Figure match at (69817, 69820)\n",
      "Figure match at (93828, 93831)\n",
      "Reference match at (92509, 92519)\n",
      "Section header \"\u001b[43m\u001b[1mINTRODUCTION\u001b[0m\" found at (1984, 1996)\n",
      "Section header \"\u001b[43m\u001b[1mPOLICY BACKGROUND\u001b[0m\" found at (5842, 5859)\n",
      "Section header \"\u001b[43m\u001b[1mLITERATURE REVIEW\u001b[0m\" found at (15602, 15619)\n",
      "Section header \"\u001b[43m\u001b[1mDATA AND METHODS\u001b[0m\" found at (21550, 21566)\n",
      "Section header \"\u001b[43m\u001b[1mRESULTS\u001b[0m\" found at (32317, 32324)\n",
      "Section header \"\u001b[43m\u001b[1mCONCLUSION\u001b[0m\" found at (81943, 81953)\n",
      "Section header \"\u001b[43m\u001b[1mAPPENDIX TABLE\u001b[0m A1\" found at (90519, 90536)\n",
      "\n",
      "-----------\n",
      "||Article||\n",
      "-----------\n",
      "Moving Teenagers Out of High-Risk\n",
      "Neighborhoods: How Girls Fare Better than\n",
      "Boys1 Susan Clampet-Lundquist\n",
      "St. Joseph’s University Kathryn Edin\n",
      "Harvard University Jeffrey R. Kling\n",
      "U.S. Congressional Budget Ofﬁce Greg J. Duncan\n",
      "University of California,\n",
      "Irvine Moving to Opportunity (MTO) offered public housing residents the\n",
      "opportunity to move to low-poverty neighborhoods. Several years\n",
      "later, boys in the experimental group fared no better on measures of\n",
      "risk behavior than their control group counterparts, whereas girls in\n",
      "the experimental group engaged in lower-risk behavior than control\n",
      "group girls. The authors explore these differences by analyzing data\n",
      "from in-depth interviews conducted with 86 teens in Baltimore and\n",
      "Chicago. They ﬁnd that daily routines, ﬁtting in with neighborhood\n",
      "norms, neighborhood navigation strategies, interactions with peers,\n",
      "friendship making, and distance from father ﬁgures may contribute\n",
      "to how girls who moved via MTO beneﬁted more than boys. \u001b[43m\u001b[1mINTRODUCTION\u001b[0m Young people who grow up in high-poverty urban neighborhoods expe-\n",
      "rience crime and violence, resource-poor schools, restricted labor markets, 1 Primary support for this research was provided by grants from the Russell Sage\n",
      "Foundation and the William T. Grant Foundation. The views expressed in this article\n",
      "are those of the authors and should not be interpreted as those of the Congressional\n",
      "Budget Ofﬁce. We are grateful to Todd Richardson and Mark Shroder of the De-\n",
      "partment of Housing and Urban Development; to Eric Beecroft, Judie Feins, Barbara\n",
      "Goodson, Robin Jacob, Stephen Kennedy, Larry Orr, and Rhiannon Patterson of Abt\n",
      "Associates; to our collaborators Jeanne Brooks-Gunn, Lawrence Katz, Tama Leven-\n",
      "thal, Jeffrey Liebman, Jens Ludwig, and Lisa Sanbonmatsu; and to staff members of (cid:1) 2011 by The University of Chicago. All \n"
     ]
    }
   ],
   "source": [
    "with open('example.txt', 'r') as f:\n",
    "    d = f.read()\n",
    "pattern = re.compile(r'[A-Z]{4,}(\\s+?[A-Z]{2,}){0,}')\n",
    "print(highlight(pattern, d)[:3000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## American Journal of Sociology articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [01:32<00:00,  1.47s/it]\n",
      "100%|██████████| 37/37 [00:44<00:00,  1.19s/it]\n",
      "100%|██████████| 68/68 [02:48<00:00,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 54s, sys: 2.66 s, total: 4min 57s\n",
      "Wall time: 5min 4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# AJS articles - split into 3 periods\n",
    "\n",
    "pre1946 = '/Users/Praveens/Desktop/ishan/Language-of-Science/articles/AJS pdf files/pre1946/'\n",
    "pre1946pdfs = os.listdir(pre1946) # list of all the pdf files \n",
    "pre1946pdfs.sort() # sort by year (and title)\n",
    "\n",
    "l946to1966 = '/Users/Praveens/Desktop/ishan/Language-of-Science/articles/AJS pdf files/1946to1966/'\n",
    "l946to1966pdfs = os.listdir(l946to1966) # list of all the pdf files\n",
    "l946to1966pdfs.sort()\n",
    "\n",
    "post1971 = '/Users/Praveens/Desktop/ishan/Language-of-Science/articles/AJS pdf files/post1971/'\n",
    "post1971pdfs = os.listdir(post1971) # list of all the pdf files\n",
    "post1971pdfs.sort()\n",
    "\n",
    "# Convert the articles to strings... this is the time-consuming step\n",
    "convertedStrings_pre1946 = [PDFtoString(os.path.join(pre1946, file)) for file in tqdm(pre1946pdfs) if file[-4:] == '.pdf']\n",
    "convertedStrings_1946to1966 = [PDFtoString(os.path.join(l946to1966, file)) for file in tqdm(l946to1966pdfs) if file[-4:] == '.pdf']\n",
    "convertedStrings_post1971 = [PDFtoString(os.path.join(post1971, file)) for file in tqdm(post1971pdfs) if file[-4:] == '.pdf']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# American Sociological Review articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:13<00:00,  1.20it/s]\n",
      "100%|██████████| 121/121 [04:11<00:00,  2.08s/it]\n"
     ]
    }
   ],
   "source": [
    "ASRpre1946 = '/Users/Praveens/Desktop/ishan/Language-of-Science/articles/ASR pdf files/pre1946/'\n",
    "ASRpre1946pdfs = os.listdir(ASRpre1946) # list of all the pdf files \n",
    "ASRpre1946pdfs.sort() # sort by year (and title)\n",
    "\n",
    "ASRpost1946 = '/Users/Praveens/Desktop/ishan/Language-of-Science/articles/ASR pdf files/post1946/'\n",
    "ASRpost1946pdfs = os.listdir(ASRpost1946) # list of all the pdf files\n",
    "ASRpost1946pdfs.sort()\n",
    "\n",
    "# Convert the articles to strings... this is the time-consuming step\n",
    "convertedStrings_ASRpre1946 = [PDFtoString(os.path.join(ASRpre1946, file)) for file in tqdm(ASRpre1946pdfs)]\n",
    "convertedStrings_ASRpost1946 = [PDFtoString(os.path.join(ASRpost1946, file)) for file in tqdm(ASRpost1946pdfs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the double lines / extra space characters & footer download / use notice\n",
    "\n",
    "convertedStrings_pre1946 = [' '.join(re.split('\\n\\n+', string)) for string in convertedStrings_pre1946]\n",
    "convertedStrings_pre1946 = [' '.join(re.split(r'This content downloaded from (.)+\\n(.)+\\x0c', string)) for string in convertedStrings_pre1946]\n",
    "\n",
    "convertedStrings_1946to1966 = [' '.join(re.split('\\n\\n+', string)) for string in convertedStrings_1946to1966]\n",
    "convertedStrings_1946to1966 = [' '.join(re.split(r'This content downloaded from (.)+\\n(.)+\\x0c', string)) for string in convertedStrings_1946to1966]\n",
    "\n",
    "convertedStrings_post1971 = [' '.join(re.split('\\n\\n+', string)) for string in convertedStrings_post1971]\n",
    "convertedStrings_post1971 = [' '.join(re.split(r'This content downloaded from (.)+\\n(.)+\\x0c', string)) for string in convertedStrings_post1971]\n",
    "\n",
    "# Optional text highlighting:\n",
    "### pattern = re.compile(r'This content downloaded from (.)+\\n(.)+\\x0c')\n",
    "### highlight(pattern, convertedStrings_pre1946[4])\n",
    "### printMetaInfo(convertedStrings_pre1946, pdfPaths = pre1946pdfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing the files out to .txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:00, 3244.21it/s]\n",
      "37it [00:00, 3107.45it/s]\n",
      "68it [00:00, 2839.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n",
      "done!\n",
      "done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "out_pre1946 = createOutputStrings(convertedStrings_pre1946, folder = pre1946pdfs)\n",
    "out_1946to1966 = createOutputStrings(convertedStrings_1946to1966, folder = l946to1966pdfs)\n",
    "out_post1971 = createOutputStrings(convertedStrings_post1971, folder = post1971pdfs)\n",
    "\n",
    "writeOut(out_pre1946, pdfFolder = pre1946pdfs, outFolder = 'AJS_pre1946')\n",
    "writeOut(out_1946to1966, pdfFolder = l946to1966pdfs, outFolder = 'AJS_1946to1966')\n",
    "writeOut(out_post1971, pdfFolder = post1971pdfs, outFolder = 'AJS_post1971')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consolidating PyTesseract output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, for a few test cases - the first two files of AJS_pre1946\n",
    "tess_bF = 'tesseract-corpus-raw'\n",
    "journalFolder= 'AJS_pre1946'\n",
    "\n",
    "# For each folder in the journalFolders, find the out.txt file and copy it to the corpus w/ the name of the folder\n",
    "for folder in os.listdir(os.path.join(tess_bF, journalFolder))[:2]: # First two files for now\n",
    "    for file in os.listdir(os.path.join(tess_bF, journalFolder, folder)):\n",
    "        if file[-4:] == '.txt':\n",
    "            outFile = open(os.path.join('tesseract-corpus', journalFolder, folder) + '.txt', 'w')\n",
    "            srcFile = open(os.path.join(tess_bF, journalFolder, folder, file), 'r')\n",
    "            shutil.copyfileobj(srcFile, outFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, cycle through each journal and do the same\n",
    "journalFolders = ['AJS_pre1946', 'ASR_pre1946', 'ASR_post1946', 'AJS_1946to1966', 'AJS_post1971']\n",
    "\n",
    "# For each folder in the journalFolders, find the out.txt file and copy it to the corpus w/ the name of the folder\n",
    "for journal in journalFolders:\n",
    "    for folder in os.listdir(os.path.join(tess_bF, journal)):\n",
    "        if folder[0] != '.':\n",
    "            for file in os.listdir(os.path.join(tess_bF, journal, folder)):\n",
    "                if file[-4:] == '.txt':\n",
    "                    outFile = open(os.path.join('tesseract-corpus', journal, folder) + '.txt', 'w')\n",
    "                    srcFile = open(os.path.join(tess_bF, journal, folder, file), 'r')\n",
    "                    shutil.copyfileobj(srcFile, outFile) \n",
    "                    \n",
    "# Resulting output is the corpus .txt files in tesseract-corpus nicely sectioned into each of the journal folders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run spell checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22min 24s, sys: 815 ms, total: 22min 25s\n",
      "Wall time: 22min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "spell = Speller('en')\n",
    "AJSpre1946strings = [spell(string) for string in convertedStrings_pre1946]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "AJS1944to46strings = [spell(string) for string in convertedStrings_1946to1966]\n",
    "AJSpost1971strings = [spell(string) for string in convertedStrings_post1971]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-insert section headers into Tesseract output based on newly converted strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example string\n",
    "indx = 0\n",
    "string = AJSpre1946strings[5]\n",
    "journal = 'AJS'\n",
    "folder = pre1946pdfs\n",
    "\n",
    "#Lookback parameter\n",
    "lookback = 3\n",
    "\n",
    "### First, look for the title words and tokenize them.\n",
    "if journal == 'AJS':\n",
    "    titles = [re.findall(r'(?<=AJS_\\d{4}_\\d{1,3}_\\d{1,2}_).*(?=.pdf)', file_name)[0] for file_name in folder]\n",
    "elif journal == 'ASR':\n",
    "    titles = [re.findall(r'(?<=ASR_\\d{4}_\\d{1,3}_\\d{1,2}_).*(?=.pdf)', file_name)[0] for file_name in folder]\n",
    "title_words = [i for i in titles[indx].upper().split() if i not in stop_words and not i.isdigit()]\n",
    "\n",
    "### Then add them to stop words & other hard-coded regularly occuring words\n",
    "non_section_words = hard_coded_non_section_words + title_words + stop_words\n",
    "matches = re.finditer(r'[A-Z]{4,}(\\s+?[A-Z]{2,}){0,}', string)\n",
    "\n",
    "if matches is not None:\n",
    "    sections = dict()\n",
    "    for match in matches:\n",
    "        if match.group() not in non_section_words and match not in sections:\n",
    "            sections[match.group()] = [match.span(), string[match.start()-lookback:match.start()], string[match.end():match.end()+lookback]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.listdir('tesseract-corpus')\n",
    "\n",
    "tess_out = dict([(journal, list()) for journal in journalFolders])\n",
    "for journal in journalFolders: # [AJS_pre1946, ASR_pre1946, ASR_post1946, AJS_1946to1966, AJS_post1971]\n",
    "    convertedTxtFiles = os.listdir(os.path.join('tesseract-corpus', journal))\n",
    "    convertedTxtFiles.sort()\n",
    "    for indx, file in enumerate(convertedTxtFiles): # First five files\n",
    "        tess_file = open(os.path.join('tesseract-corpus', journal, file), 'r').read()\n",
    "        tess_out[journal].append(tess_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spellchecker on Tesseract output, too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16min 53s, sys: 374 ms, total: 16min 54s\n",
      "Wall time: 16min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "AJSpre1946tess = [spell(string) for string in tess_out['AJS_pre1946']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 48s, sys: 114 ms, total: 5min 48s\n",
      "Wall time: 5min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "AJS1944to46tess = [spell(string) for string in tess_out['AJS_1946to1966']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 48min 39s, sys: 1.07 s, total: 48min 40s\n",
      "Wall time: 48min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "AJSpost1971tess = [spell(string) for string in tess_out['AJS_post1971']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 7s, sys: 65 ms, total: 3min 7s\n",
      "Wall time: 3min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ASRpre1946tess = [spell(string) for string in tess_out['ASR_pre1946']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 41min 1s, sys: 873 ms, total: 41min 2s\n",
      "Wall time: 41min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ASRpost1946tess = [spell(string) for string in tess_out['ASR_post1946']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [00:00, 3623.86it/s]\n",
      "37it [00:00, 3503.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n",
      "done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "68it [00:00, 3058.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "out_AJSpre1946 = createOutputStrings(AJSpre1946tess, folder = pre1946pdfs, journal = 'AJS')\n",
    "writeOut(out_AJSpre1946, pdfFolder = pre1946pdfs, outFolder = 'AJS_pre1946')\n",
    "\n",
    "out_AJS1946to66 = createOutputStrings(AJS1944to46tess, folder = l946to1966pdfs, journal = 'AJS')\n",
    "writeOut(out_AJS1946to66, pdfFolder = l946to1966pdfs, outFolder = 'AJS_1946to1966')\n",
    "\n",
    "out_AJSpost1971 = createOutputStrings(AJSpost1971tess, folder = post1971pdfs, journal = 'AJS')\n",
    "writeOut(out_AJSpost1971, pdfFolder = post1971pdfs, outFolder = 'AJS_post1971')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [00:00, 1708.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "121it [00:00, 2809.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "out_ASRpre1946 = createOutputStrings(ASRpre1946tess, folder = ASRpre1946pdfs, journal = 'ASR')\n",
    "writeOut(out_ASRpre1946, pdfFolder = ASRpre1946pdfs, outFolder = 'ASR_pre1946')\n",
    "\n",
    "out_ASRpost1946 = createOutputStrings(ASRpost1946tess, folder = ASRpost1946pdfs, journal = 'ASR')\n",
    "writeOut(out_ASRpost1946, pdfFolder = ASRpost1946pdfs, outFolder = 'ASR_post1946')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
